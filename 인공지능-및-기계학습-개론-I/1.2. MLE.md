# MLE
url
* https://kaist.edwith.org/machinelearning1_17/lecture/10575/
## Thumbtack Question
* 압정 문제의 경우, 동전처럼 쉽게 앞면 뒷면 확률이 반반이라고 정의 내리기 어려움
* 그렇다면 몇 번씩 던져보자!
  * 앞면 3/5, 뒷면 2/5 나옴
  * 왜?
  
## Binomial Distribution
* T/F, H/T 이산적인 확률 분포 → 압정을 던지면 H/F에 대한 확률 분포가 나옴
  * 2가지의 케이스만 존재 → binomial 
* is the **discrete** probability 
* Bernoulli experiment → 여러번 던져보면서 확인
* flips are **i.i.d**
  * i.i.d (독립항등분포) = indepentdent and identically distributed random variables
  * 앞에서 던졌을 때랑 뒤에서 던졌을 때 이벤트가 서로 연관이 없다
* P(H) = θ, P(T) = 1-θ
  * 확률은 항상 양수
  * 0 ≤ θ ≤ 1
* 그렇다면 D (데이터)를 관측할 확률은?

![image](https://user-images.githubusercontent.com/42960718/83943051-e3fc9080-a833-11ea-95b0-4e89c91a4fc4.png)


## Maximum Likelihood Estimation
* 압정은 θ 가정 → 그렇다면 어떤 θ를 선정해야 이 데이터를 가장 잘 설명할 수 있을까?
 * **best candidate of θ**
* one candidate is MLE of θ
* Maximum Likelihood Estimation (MLE) = 관측된 데이터들의 등장할 확률을 최대화하는 θ 찾는 것

 ![image](https://user-images.githubusercontent.com/42960718/83943148-959bc180-a834-11ea-96c1-a1fb93dfae57.png)
* θ가 주어졌을 때 데이터가 관측될 확률을 알 수 있고, 이것을 최대화 (argmax)하는 θ를 찾아내자
* 그 θ를 **theta hat**이라고 정의

## MLE Calculation
* log 값을 붙여 복잡한 자승 공식을 쉽게 만든다 → log 함수는 monotnoic하게 증가하기 때문에 확률 함수를 log를 활용해서 mapping하면 P 최대화 값과 log 최대화 값이 같음
 * 값은 각자 다르지만 최대화되는 값은 같음
 
  ![image](https://user-images.githubusercontent.com/42960718/83943251-8701da00-a835-11ea-81e7-fb846afa5c5b.png)

## Simple Error Bound
* 실험을 반복했던 횟수를 늘리면 뭐가 좋은가?
 * 우리가 측정하고 있는 것은 theta 자체가 아니라 **theta hat**임 → parameter에 대한 **추론**을 진행한 것, 진짜 paramter를 측정한게 아님

![image](https://user-images.githubusercontent.com/42960718/83943354-4e163500-a836-11ea-94f8-5da4a1212e43.png)

 * 이런 paramter 측정에 대한 **error**가 줄어든 것
 * error bound, N이 커지면 커질수록, error가 발생할 확률이 낮아짐
* Probability Approximately Correct Learning (PAC)

![image](https://user-images.githubusercontent.com/42960718/83943386-9df4fc00-a836-11ea-9f9e-9fe263e61aa2.png)




  



  

  
  
